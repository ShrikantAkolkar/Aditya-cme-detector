{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aditya-L1 CME Detection - Threshold Optimization\n",
    "\n",
    "This notebook focuses on optimizing detection thresholds for CME events using statistical analysis and machine learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "print(\"Threshold Optimization for Aditya-L1 CME Detection\")\n",
    "print(\"=\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (using simulation for demonstration)\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.data_ingestion.swis_ingestion import SWISIngestion\n",
    "from src.data_ingestion.cactus_ingestion import CACTUSIngestion\n",
    "from src.processing.feature_engineering import FeatureEngineer\n",
    "from src.config import config\n",
    "\n",
    "# Generate sample data\n",
    "swis_ingestion = SWISIngestion()\n",
    "cactus_ingestion = CACTUSIngestion()\n",
    "\n",
    "# Simulate 30 days of data for better statistics\n",
    "particle_data = swis_ingestion.simulate_particle_data(duration_minutes=30*24*60)\n",
    "particle_df = pd.DataFrame(particle_data)\n",
    "particle_df['timestamp'] = pd.to_datetime(particle_df['timestamp'])\n",
    "\n",
    "cme_events = cactus_ingestion.simulate_cme_events(days=30)\n",
    "cme_df = pd.DataFrame(cme_events)\n",
    "cme_df['timestamp'] = pd.to_datetime(cme_df['timestamp'])\n",
    "\n",
    "print(f\"Loaded {len(particle_df)} particle data points and {len(cme_df)} CME events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer features\n",
    "feature_engineer = FeatureEngineer(config._config)\n",
    "features_df = feature_engineer.engineer_features(particle_df)\n",
    "\n",
    "print(f\"Engineered features shape: {features_df.shape}\")\n",
    "print(f\"Feature columns: {len(features_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Labels for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary labels for CME detection\n",
    "from datetime import timedelta\n",
    "\n",
    "def create_cme_labels(features_df, cme_df, window_minutes=30):\n",
    "    \"\"\"\n",
    "    Create binary labels for CME detection\n",
    "    \"\"\"\n",
    "    labels = np.zeros(len(features_df))\n",
    "    \n",
    "    for _, cme_event in cme_df.iterrows():\n",
    "        cme_time = cme_event['timestamp']\n",
    "        \n",
    "        # Define time window around CME event\n",
    "        start_time = cme_time - timedelta(minutes=window_minutes)\n",
    "        end_time = cme_time + timedelta(minutes=window_minutes)\n",
    "        \n",
    "        # Mark data points within this window as CME events\n",
    "        mask = (features_df['timestamp'] >= start_time) & (features_df['timestamp'] <= end_time)\n",
    "        labels[mask] = 1\n",
    "    \n",
    "    return labels\n",
    "\n",
    "# Create labels\n",
    "labels = create_cme_labels(features_df, cme_df)\n",
    "\n",
    "print(f\"Created labels: {len(labels)} total, {np.sum(labels)} CME events ({np.mean(labels)*100:.2f}%)\")\n",
    "print(f\"Class distribution: {np.bincount(labels.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize single parameter thresholds\n",
    "def optimize_single_threshold(feature_values, labels, metric='f1'):\n",
    "    \"\"\"\n",
    "    Optimize threshold for a single feature\n",
    "    \"\"\"\n",
    "    thresholds = np.percentile(feature_values, np.linspace(50, 99, 50))\n",
    "    scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = (feature_values >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp = np.sum((predictions == 1) & (labels == 1))\n",
    "        fp = np.sum((predictions == 1) & (labels == 0))\n",
    "        fn = np.sum((predictions == 0) & (labels == 1))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        if metric == 'f1':\n",
    "            scores.append(f1)\n",
    "        elif metric == 'precision':\n",
    "            scores.append(precision)\n",
    "        elif metric == 'recall':\n",
    "            scores.append(recall)\n",
    "    \n",
    "    best_idx = np.argmax(scores)\n",
    "    return thresholds[best_idx], scores[best_idx], thresholds, scores\n",
    "\n",
    "# Test key features\n",
    "key_features = ['proton_flux', 'velocity', 'temperature', 'magnetic_field_magnitude']\n",
    "threshold_results = {}\n",
    "\n",
    "for feature in key_features:\n",
    "    if feature in features_df.columns:\n",
    "        best_threshold, best_score, thresholds, scores = optimize_single_threshold(\n",
    "            features_df[feature].values, labels\n",
    "        )\n",
    "        threshold_results[feature] = {\n",
    "            'best_threshold': best_threshold,\n",
    "            'best_score': best_score,\n",
    "            'thresholds': thresholds,\n",
    "            'scores': scores\n",
    "        }\n",
    "        print(f\"{feature}: Best threshold = {best_threshold:.2f}, F1 score = {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot threshold optimization curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Single Feature Threshold Optimization', fontsize=16)\n",
    "\n",
    "for i, (feature, results) in enumerate(threshold_results.items()):\n",
    "    row, col = i // 2, i % 2\n",
    "    \n",
    "    axes[row, col].plot(results['thresholds'], results['scores'], 'b-', linewidth=2)\n",
    "    axes[row, col].axvline(results['best_threshold'], color='red', linestyle='--', \n",
    "                          label=f'Best: {results[\"best_threshold\"]:.2f}')\n",
    "    axes[row, col].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "    axes[row, col].set_xlabel('Threshold')\n",
    "    axes[row, col].set_ylabel('F1 Score')\n",
    "    axes[row, col].legend()\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Parameter Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-parameter threshold optimization\n",
    "def multi_threshold_objective(thresholds, features_matrix, labels, weights=None):\n",
    "    \"\"\"\n",
    "    Objective function for multi-parameter threshold optimization\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(thresholds))\n",
    "    \n",
    "    # Apply thresholds\n",
    "    predictions = np.zeros(len(labels))\n",
    "    \n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        feature_predictions = (features_matrix[:, i] >= threshold).astype(float)\n",
    "        predictions += weights[i] * feature_predictions\n",
    "    \n",
    "    # Convert to binary predictions\n",
    "    predictions = (predictions >= np.sum(weights) * 0.5).astype(int)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    tp = np.sum((predictions == 1) & (labels == 1))\n",
    "    fp = np.sum((predictions == 1) & (labels == 0))\n",
    "    fn = np.sum((predictions == 0) & (labels == 1))\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return -f1  # Negative because we're minimizing\n",
    "\n",
    "# Prepare feature matrix\n",
    "selected_features = ['proton_flux', 'velocity', 'temperature']\n",
    "available_features = [f for f in selected_features if f in features_df.columns]\n",
    "\n",
    "if available_features:\n",
    "    features_matrix = features_df[available_features].values\n",
    "    \n",
    "    # Initial thresholds (median values)\n",
    "    initial_thresholds = [np.median(features_matrix[:, i]) for i in range(len(available_features))]\n",
    "    \n",
    "    # Bounds for optimization (10th to 90th percentile)\n",
    "    bounds = [(np.percentile(features_matrix[:, i], 10), \n",
    "              np.percentile(features_matrix[:, i], 90)) \n",
    "             for i in range(len(available_features))]\n",
    "    \n",
    "    # Optimize\n",
    "    result = minimize(\n",
    "        multi_threshold_objective,\n",
    "        initial_thresholds,\n",
    "        args=(features_matrix, labels),\n",
    "        bounds=bounds,\n",
    "        method='L-BFGS-B'\n",
    "    )\n",
    "    \n",
    "    optimal_thresholds = result.x\n",
    "    optimal_f1 = -result.fun\n",
    "    \n",
    "    print(\"Multi-parameter Optimization Results:\")\n",
    "    print(f\"Optimal F1 Score: {optimal_f1:.3f}\")\n",
    "    for i, feature in enumerate(available_features):\n",
    "        print(f\"{feature}: {optimal_thresholds[i]:.2f}\")\n",
    "else:\n",
    "    print(\"No suitable features available for multi-parameter optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ROC and Precision-Recall Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and PR curve analysis\n",
    "def plot_roc_pr_curves(features_df, labels, feature_name):\n",
    "    \"\"\"\n",
    "    Plot ROC and Precision-Recall curves for a feature\n",
    "    \"\"\"\n",
    "    if feature_name not in features_df.columns:\n",
    "        print(f\"Feature {feature_name} not found\")\n",
    "        return\n",
    "    \n",
    "    feature_values = features_df[feature_name].values\n",
    "    \n",
    "    # ROC curve\n",
    "    fpr, tpr, roc_thresholds = roc_curve(labels, feature_values)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(labels, feature_values)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # ROC curve\n",
    "    ax1.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "    ax1.plot([0, 1], [0, 1], 'r--', alpha=0.5)\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title(f'ROC Curve - {feature_name.replace(\"_\", \" \").title()}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    ax2.plot(recall, precision, 'g-', linewidth=2, label=f'PR (AUC = {pr_auc:.3f})')\n",
    "    ax2.axhline(y=np.mean(labels), color='r', linestyle='--', alpha=0.5, label='Baseline')\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title(f'Precision-Recall Curve - {feature_name.replace(\"_\", \" \").title()}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc, pr_auc\n",
    "\n",
    "# Analyze key features\n",
    "auc_results = {}\n",
    "for feature in ['proton_flux', 'velocity']:\n",
    "    if feature in features_df.columns:\n",
    "        print(f\"\\nAnalyzing {feature}:\")\n",
    "        roc_auc, pr_auc = plot_roc_pr_curves(features_df, labels, feature)\n",
    "        auc_results[feature] = {'roc_auc': roc_auc, 'pr_auc': pr_auc}\n",
    "        print(f\"ROC AUC: {roc_auc:.3f}, PR AUC: {pr_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Threshold Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity analysis for threshold variations\n",
    "def threshold_sensitivity_analysis(features_df, labels, base_thresholds, feature_names, variation_range=0.2):\n",
    "    \"\"\"\n",
    "    Analyze sensitivity of performance to threshold variations\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, feature in enumerate(feature_names):\n",
    "        if feature not in features_df.columns:\n",
    "            continue\n",
    "            \n",
    "        base_threshold = base_thresholds[i]\n",
    "        variations = np.linspace(\n",
    "            base_threshold * (1 - variation_range),\n",
    "            base_threshold * (1 + variation_range),\n",
    "            21\n",
    "        )\n",
    "        \n",
    "        for variation in variations:\n",
    "            # Apply threshold\n",
    "            predictions = (features_df[feature].values >= variation).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            tp = np.sum((predictions == 1) & (labels == 1))\n",
    "            fp = np.sum((predictions == 1) & (labels == 0))\n",
    "            fn = np.sum((predictions == 0) & (labels == 1))\n",
    "            tn = np.sum((predictions == 0) & (labels == 0))\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'threshold': variation,\n",
    "                'threshold_ratio': variation / base_threshold,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'specificity': specificity\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Perform sensitivity analysis\n",
    "if 'optimal_thresholds' in locals() and available_features:\n",
    "    sensitivity_df = threshold_sensitivity_analysis(\n",
    "        features_df, labels, optimal_thresholds, available_features\n",
    "    )\n",
    "    \n",
    "    # Plot sensitivity results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Threshold Sensitivity Analysis', fontsize=16)\n",
    "    \n",
    "    metrics = ['precision', 'recall', 'f1', 'specificity']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        row, col = i // 2, i % 2\n",
    "        \n",
    "        for feature in available_features:\n",
    "            feature_data = sensitivity_df[sensitivity_df['feature'] == feature]\n",
    "            axes[row, col].plot(feature_data['threshold_ratio'], feature_data[metric], \n",
    "                              label=feature, linewidth=2)\n",
    "        \n",
    "        axes[row, col].axvline(1.0, color='red', linestyle='--', alpha=0.5, label='Base Threshold')\n",
    "        axes[row, col].set_title(f'{metric.title()}')\n",
    "        axes[row, col].set_xlabel('Threshold Ratio')\n",
    "        axes[row, col].set_ylabel(metric.title())\n",
    "        axes[row, col].legend()\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping sensitivity analysis - no optimal thresholds available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. False Positive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze false positives\n",
    "def analyze_false_positives(features_df, labels, thresholds, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze characteristics of false positive detections\n",
    "    \"\"\"\n",
    "    if not feature_names or len(feature_names) == 0:\n",
    "        print(\"No features available for false positive analysis\")\n",
    "        return\n",
    "    \n",
    "    # Apply thresholds to get predictions\n",
    "    predictions = np.zeros(len(labels))\n",
    "    \n",
    "    for i, feature in enumerate(feature_names):\n",
    "        if feature in features_df.columns and i < len(thresholds):\n",
    "            feature_predictions = (features_df[feature].values >= thresholds[i]).astype(float)\n",
    "            predictions += feature_predictions\n",
    "    \n",
    "    # Convert to binary\n",
    "    predictions = (predictions >= len(feature_names) * 0.5).astype(int)\n",
    "    \n",
    "    # Identify false positives\n",
    "    fp_mask = (predictions == 1) & (labels == 0)\n",
    "    tp_mask = (predictions == 1) & (labels == 1)\n",
    "    \n",
    "    print(f\"False Positive Analysis:\")\n",
    "    print(f\"Total predictions: {np.sum(predictions)}\")\n",
    "    print(f\"True positives: {np.sum(tp_mask)}\")\n",
    "    print(f\"False positives: {np.sum(fp_mask)}\")\n",
    "    print(f\"False positive rate: {np.sum(fp_mask) / np.sum(predictions) * 100:.1f}%\")\n",
    "    \n",
    "    if np.sum(fp_mask) > 0 and np.sum(tp_mask) > 0:\n",
    "        # Compare characteristics\n",
    "        fp_data = features_df[fp_mask]\n",
    "        tp_data = features_df[tp_mask]\n",
    "        \n",
    "        print(\"\\nCharacteristics comparison (FP vs TP):\")\n",
    "        for feature in feature_names:\n",
    "            if feature in features_df.columns:\n",
    "                fp_mean = fp_data[feature].mean()\n",
    "                tp_mean = tp_data[feature].mean()\n",
    "                print(f\"{feature}: FP={fp_mean:.2f}, TP={tp_mean:.2f}, Ratio={fp_mean/tp_mean:.2f}\")\n",
    "        \n",
    "        # Plot comparison\n",
    "        fig, axes = plt.subplots(1, len(feature_names), figsize=(5*len(feature_names), 4))\n",
    "        if len(feature_names) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, feature in enumerate(feature_names):\n",
    "            if feature in features_df.columns:\n",
    "                axes[i].hist(fp_data[feature], bins=20, alpha=0.7, label='False Positives', color='red')\n",
    "                axes[i].hist(tp_data[feature], bins=20, alpha=0.7, label='True Positives', color='green')\n",
    "                axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "                axes[i].set_xlabel('Value')\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "                axes[i].legend()\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Perform false positive analysis\n",
    "if 'optimal_thresholds' in locals() and available_features:\n",
    "    analyze_false_positives(features_df, labels, optimal_thresholds, available_features)\n",
    "else:\n",
    "    # Use single feature thresholds\n",
    "    single_thresholds = [threshold_results[f]['best_threshold'] for f in key_features if f in threshold_results]\n",
    "    single_features = [f for f in key_features if f in threshold_results]\n",
    "    if single_thresholds:\n",
    "        analyze_false_positives(features_df, labels, single_thresholds, single_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Recommended Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final threshold recommendations\n",
    "print(\"THRESHOLD OPTIMIZATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. SINGLE FEATURE THRESHOLDS:\")\n",
    "for feature, results in threshold_results.items():\n",
    "    print(f\"   {feature.replace('_', ' ').title()}:\")\n",
    "    print(f\"     - Optimal threshold: {results['best_threshold']:.2f}\")\n",
    "    print(f\"     - F1 score: {results['best_score']:.3f}\")\n",
    "\n",
    "if 'optimal_thresholds' in locals() and available_features:\n",
    "    print(\"\\n2. MULTI-PARAMETER THRESHOLDS:\")\n",
    "    print(f\"   Combined F1 score: {optimal_f1:.3f}\")\n",
    "    for i, feature in enumerate(available_features):\n",
    "        print(f\"   {feature.replace('_', ' ').title()}: {optimal_thresholds[i]:.2f}\")\n",
    "\n",
    "print(\"\\n3. FEATURE IMPORTANCE (by AUC):\")\n",
    "if auc_results:\n",
    "    sorted_features = sorted(auc_results.items(), key=lambda x: x[1]['roc_auc'], reverse=True)\n",
    "    for feature, aucs in sorted_features:\n",
    "        print(f\"   {feature.replace('_', ' ').title()}: ROC AUC = {aucs['roc_auc']:.3f}\")\n",
    "\n",
    "print(\"\\n4. RECOMMENDED CONFIGURATION:\")\n",
    "print(\"   For production deployment, consider:\")\n",
    "print(\"   - Use multi-parameter thresholds for better accuracy\")\n",
    "print(\"   - Implement adaptive thresholds based on solar activity\")\n",
    "print(\"   - Regular retraining with new data\")\n",
    "print(\"   - False positive monitoring and feedback loop\")\n",
    "\n",
    "# Save optimized thresholds to config format\n",
    "config_output = {\n",
    "    'detection': {\n",
    "        'thresholds': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add single feature thresholds\n",
    "for feature, results in threshold_results.items():\n",
    "    config_key = f\"{feature}_threshold\"\n",
    "    config_output['detection']['thresholds'][config_key] = float(results['best_threshold'])\n",
    "\n",
    "print(\"\\n5. CONFIGURATION OUTPUT:\")\n",
    "import yaml\n",
    "print(yaml.dump(config_output, default_flow_style=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "nbformat_minor": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}